import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Input, Multiply, Activation, Concatenate, Lambda
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras import regularizers


np.random.seed(42)
tf.random.set_seed(42)

print("Downloading S&P 500 historical data...")
sp500 = yf.download('^GSPC', start='1980-01-01', end=datetime.now().strftime('%Y-%m-%d'))
print(f"Downloaded {len(sp500)} records")

data = sp500[['Close']].copy()
data.reset_index(inplace=True)

data['MA_7'] = data['Close'].rolling(window=7).mean()
data['MA_30'] = data['Close'].rolling(window=30).mean()
data['MA_90'] = data['Close'].rolling(window=90).mean()
data['Volatility'] = data['Close'].rolling(window=20).std()
data['Return'] = data['Close'].pct_change()
data['Month'] = data['Date'].dt.month
data['Year'] = data['Date'].dt.year

# Add cyclical encoding for month
data['Month_sin'] = np.sin(2 * np.pi * data['Month']/12)
data['Month_cos'] = np.cos(2 * np.pi * data['Month']/12)

# Drop NaN values
data.dropna(inplace=True)

# Select features
features = ['Close', 'MA_7', 'MA_30', 'MA_90', 'Volatility', 'Return', 'Month_sin', 'Month_cos']
dataset = data[features].values

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)


# Create sequences for LSTM
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length, 0])  # 0 index is Close price
    return np.array(X), np.array(y)

# Define sequence length
sequence_length = 60  # Using 60 days of history

# Create sequences
X, y = create_sequences(scaled_data, sequence_length)

# Split into training and testing sets (using 80% for training)
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Add these imports if not already present
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras.layers import Dropout

def create_self_attention_lstm(sequence_length, feature_dim):
    # Input shape (sequence_length, feature_dim)
    inputs = Input(shape=(sequence_length, feature_dim))
    
    # Self-attention mechanism with regularization
    # Query, Key, Value transformations
    query = Dense(64, activation='linear', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(inputs)
    key = Dense(64, activation='linear', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(inputs)
    value = Dense(64, activation='linear', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(inputs)
    
    # Compute attention scores using Lambda layers
    def attention_score(x):
        q, k = x
        return K.batch_dot(q, K.permute_dimensions(k, (0, 2, 1))) / np.sqrt(64)
    
    score = Lambda(attention_score)([query, key])
    
    # Apply softmax to get attention weights
    attention_weights = Activation('softmax')(score)
    
    # Apply attention weights to value using Lambda layer
    def apply_attention(x):
        weights, v = x
        return K.batch_dot(weights, v)
    
    context = Lambda(apply_attention)([attention_weights, value])
    
    # Concatenate with original input for residual connection
    context = Concatenate()([inputs, context])
    
    # Add dropout for regularization
    context = Dropout(0.3)(context)
    
    # LSTM layers with regularization
    lstm1 = LSTM(100, return_sequences=True, 
                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),
                recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4))(context)
    lstm1 = Dropout(0.3)(lstm1)
    
    lstm2 = LSTM(50, 
                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),
                recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4))(lstm1)
    lstm2 = Dropout(0.2)(lstm2)
    
    # Output layer with ReLU to ensure non-negative predictions
    outputs = Dense(1, activation='relu')(lstm2)
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

# Then update your model training with more restrictive early stopping
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,  # Reduced patience
    restore_best_weights=True,
    min_delta=0.0001  # Minimum improvement required
)

# And modify your model compilation to use learning rate scheduling
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Create and compile the model
model = create_self_attention_lstm(sequence_length, len(features))
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# Add learning rate reduction
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=0.00001,
    verbose=1
)


history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping, model_checkpoint, reduce_lr],
    verbose=1
)


# Evaluate the model
train_loss = model.evaluate(X_train, y_train, verbose=0)
test_loss = model.evaluate(X_test, y_test, verbose=0)
print(f"Train Loss: {train_loss:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Plot training history
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.savefig('model_training_history.png')
plt.close()


# Custom loss function to penalize dramatic changes
def smooth_prediction_loss(y_true, y_pred):
    # Standard MSE loss
    mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
    
    # Add penalty for excessive price changes during inference
    # This only affects the training process
    return mse_loss

# Or implement a smoother prediction function for inference
def predict_next_12_months_smooth(model, last_sequence, scaler, feature_count=len(features), momentum_factor=0.5):
    predictions = []
    current_sequence = last_sequence.copy()
    last_actual_close = scaler.inverse_transform(current_sequence[-1].reshape(1, -1))[0, 0]
    prev_prediction = last_actual_close
    
    # Generate 365 days of predictions (approximately 12 months)
    for _ in range(365):
        # Reshape for prediction
        current_input = current_sequence.reshape(1, sequence_length, feature_count)
        
        # Get prediction
        pred = model.predict(current_input, verbose=0)[0, 0]
        
        # Create a dummy row for inverting the scaling
        dummy_array = np.zeros((1, feature_count))
        dummy_array[0, 0] = pred
        actual_pred = scaler.inverse_transform(dummy_array)[0, 0]
        
        # Apply smoothing to limit extreme changes
        # This uses an exponential smoothing approach
        smooth_pred = prev_prediction + momentum_factor * (actual_pred - prev_prediction)
        
        # Make sure prediction doesn't fall too fast (e.g., max 3% drop per month)
        max_drop_rate = 0.03  # Maximum 3% drop per month, adjust as needed
        min_allowed = prev_prediction * (1 - max_drop_rate)
        
        # Apply constraint
        if smooth_pred < min_allowed:
            smooth_pred = min_allowed
        
        # Convert back to scaled value for next sequence
        dummy_array[0, 0] = smooth_pred
        scaled_smooth_pred = scaler.transform(dummy_array)[0, 0]
        
        # Add smoothed prediction to list
        predictions.append(smooth_pred)
        prev_prediction = smooth_pred
        
        # Create a dummy row for the next prediction sequence
        next_row = current_sequence[-1].copy()
        next_row[0] = scaled_smooth_pred  # Use smoothed prediction
        
        # Update the rest of the features as before
        # Simple updates for moving averages and other features
        next_row[1] = (scaled_smooth_pred + np.sum(current_sequence[-6:, 0])) / 7  # MA_7
        next_row[2] = (scaled_smooth_pred + np.sum(current_sequence[-29:, 0])) / 30  # MA_30
        next_row[3] = (scaled_smooth_pred + np.sum(current_sequence[-89:, 0])) / 90  # MA_90
        next_row[4] = np.std(np.append(current_sequence[-19:, 0], scaled_smooth_pred))  # Volatility
        next_row[5] = (scaled_smooth_pred / current_sequence[-1, 0]) - 1  # Return
        
        # Update month features
        day_count = _ + 1
        months_passed = data['Date'].iloc[-1].month + (day_count // 30)
        current_month = ((months_passed - 1) % 12) + 1
        next_row[6] = np.sin(2 * np.pi * current_month / 12)  # Month_sin
        next_row[7] = np.cos(2 * np.pi * current_month / 12)  # Month_cos
        
        # Update the sequence by removing the first row and adding the new prediction
        current_sequence = np.vstack([current_sequence[1:], next_row])
    
    return predictions

# Alternative solution: Use ensemble methods
def ensemble_prediction(models, last_sequence, scaler, feature_count=len(features)):
    """
    Make predictions using multiple models and average the results
    """
    all_predictions = []
    
    # Get predictions from each model
    for model in models:
        preds = predict_next_12_months_smooth(model, last_sequence, scaler, feature_count)
        all_predictions.append(preds)
    
    # Average predictions from all models
    ensemble_preds = np.mean(all_predictions, axis=0)
    return ensemble_preds

# Use historical trend as a baseline
def add_historical_trend_model(predictions, last_actual_close):
    """
    Blend model predictions with historical average trend
    """
    # Calculate historical average monthly change (e.g. from last 5 years)
    # This is an example - you would calculate this from your data
    historical_monthly_change = 0.005  # 0.5% monthly growth on average
    
    historical_predictions = []
    current_price = last_actual_close
    
    # Generate trend-based predictions
    for i in range(365):
        current_price *= (1 + historical_monthly_change/30)  # Daily compounding
        historical_predictions.append(current_price)
    
    # Blend model predictions with historical trend (70% model, 30% trend)
    blend_factor = 0.7
    blended_predictions = [
        blend_factor * pred + (1 - blend_factor) * hist 
        for pred, hist in zip(predictions, historical_predictions)
    ]
    
    return blended_predictions




last_sequence = scaled_data[-sequence_length:]


predictions = predict_next_12_months_smooth(model, last_sequence, scaler)


monthly_predictions = [predictions[i] for i in range(0, len(predictions), 30)][:12]


last_date = data['Date'].iloc[-1]
next_dates = [(last_date + timedelta(days=30*(i+1))).strftime('%Y-%m-%d') for i in range(12)]


print("\nS&P 500 Predictions for the Next 12 Months:")
print("=" * 50)
for date, pred in zip(next_dates, monthly_predictions):
    print(f"{date}: ${pred:.2f}")
